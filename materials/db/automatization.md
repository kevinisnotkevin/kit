# Автоматизация работы с базами данных

## 1.1 Введение

Качественно структурированные БД, функционирующие без сбоев, открывают множество возможностей для решения задач, не требуя дополнительных затрат времени и человеческих ресурсов. В автоматизированных БД поиск нужной информации осуществляется без особых усилий, при этом имеется ряд дополнительных функций, таких как учет клиентов, контроль финансовых и товарно-материальных потоков, создание детальной статистики, ведение документооборота и другие.

Благодаря автоматизации БД руководитель всегда может получить доступ к актуальной информации о функционировании своего бизнеса. Кроме того, использование качественных систем управления БД позволяет существенно снизить пиковые нагрузки на серверы компании, равномерно распределяя их. В результате снижаются затраты на электроэнергию и обслуживание техники.

## 1.2 ORM (Object-Relational Mapping)

ORM (Object-Relational Mapping) - это технология, которая позволяет автоматически преобразовывать данные из базы данных в объекты в коде приложения. Это позволяет разработчикам работать с данными в более удобном и понятном формате, не заботясь о деталях взаимодействия с базой данных.

Существует множество ORM-библиотек для различных языков программирования. Например, для Python есть SQLAlchemy и Django ORM, для Ruby - Active Record, для Java - Hibernate, а для JavaScript - Sequelize и TypeORM.

Применение ORM имеет множество преимуществ:

1. **Уменьшение кода:** Код для работы с БД стандартизирован и автоматизирован, что позволяет писать меньше кода на SQL.
2. **Обеспечение безопасности:** ORM обычно защитит ваше приложение от таких угроз, как SQL-инъекции.
3. **Совместимость:** Они предоставляют абстракцию, которая может позволить вашему коду работать с несколькими типами баз данных без изменений.
4. **Объектно-ориентированность:** ORM позволяет работать с БД так, как если бы данные были объектами в ООП. Это способствует расширению возможностей синтаксиса языка программирования и упрощает разработку.

Однако следует помнить, что, несмотря на все преимущества, ORM также может добавить дополнительную сложность и уменьшить производительность, особенно при работе с большими объемами данных или сложными запросами в базу данных. В этих случаях напрямую написанный SQL может быть более подходящим.

## 1.3 ETL (Extract, Transform, Load)

ETL (Extract, Transform, Load) - это процесс, который включает в себя извлечение данных из различных источников, их преобразование и загрузку в целевую базу данных. Это позволяет автоматизировать процесс сбора и обработки данных из различных источников.

**ETL-процессы** - это серия действий, которые нужны для извлечения данных из нескольких источников, их преобразования и загрузки в целевую базу данных.

Сначала происходит извлечение данных из разных источников, например, из баз данных, электронных таблиц и т.д. После того, как данные были извлечены из исходных источников, они переходят на следующий этап: их нужно преобразовать в нужный формат для дальнейшего использования. Зачастую информация, которую мы получаем на первом этапе, не всегда подходит для дальнейшей обработки, поэтому нам нужно произвести "трансформацию" данных. И в завершении - мы загружаем наши готовые данные в целевую базу данных.

### 1.3.1 Как реализовано ETL

Прежде всего, необходимо определить основную цель использования ETL. Например, мы можем стремиться выгрузить данные из базы данных, обработать их (включая очистку данных) и загрузить обратно в базу данных.

Второй шаг - выбор подходящих инструментов. Обычно для ETL используются специализированные инструменты, такие как Apache NiFi, Talend, Apache Spark, и др. Однако, в некоторых случаях можно также использовать стандартные инструменты СУБД, такие как PostgreSQL или Oracle.

Далее, мы переходим к настройке нашего ETL процесса. Первым шагом является выбор источника данных. Это может быть CSV файл, база данных или даже API. Затем мы создаем запрос для выборки данных, необходимых для проведения операций над исходными данными.

Затем мы обрабатываем исходные данные. Этот шаг может включать фильтрацию исходных данных, трансформацию данных и их очистку. Для этого можно использовать мощный SQL или специальные инструменты предварительной обработки данных, такие как OpenRefine и DataWrangler.

После этого, мы загружаем данные в целевую базу данных. Для вставки данных в базу данных следует использовать обычный SQL запрос. Важно создать таблицы заранее в базе данных и удостовериться в правильном соответствии столбцов.

И, наконец, мы можем выполнить последний шаг настройки нашего ETL процесса - обновление данных в целевой базе данных. Здесь мы можем обновлять данные, уже существующие в таблице. Для этого можно использовать оператор ON CONFLICT для вставки новых данных или обновления существующих записей в таблице.

Таким образом, это общие шаги, необходимые для реализации ETL процесса. Естественно, реализация может быть более сложной в случае работы с большими и сложными данными.

### 1.3.2 Инструменты

Давайте начнем с Apache Kafka. Этот инструмент является одним из наиболее популярных решений для обработки потоковых данных. Он позволяет синхронизировать сообщения между различными источниками и обрабатывать их в реальном времени. Кроме того, Kafka обладает мощными возможностями масштабирования, что делает его идеальным выбором для крупных проектов. Он также хорошо интегрируется с другими инструментами Big Data, такими как Apache Hadoop и Spark.

Следующим в списке является Apache NiFi. Этот инструмент был разработан компанией NSA (Национальное Управление США по Вопросам Безопасности) почти десять лет назад и сейчас является проектом Apache Foundation. NiFi может легко обрабатывать большие объемы данных, а его графический интерфейс обеспечивает удобный и интуитивно понятный интерфейс для определения преобразований данных. Он также обеспечивает автоматическое масштабирование обработки данных и имеет множество источников данных, включая базы данных, файлы и даже API.

Ну и, наконец, не могу не упомянуть Apache Spark. Spark - это быстрый и мощный инструмент обработки данных, который может использоваться для решения множества задач, включая ETL. Spark поддерживает работу с данными в режиме реального времени и потоковую обработку данных, что делает его отличным выбором для большинства проектов обработки данных. Он также умеет работать с различными источниками данных и обеспечивает быстрый и масштабируемый процесс ETL.

## 1.4 Хранимые процедуры

Хранимые процедуры: Хранимые процедуры - это блоки кода, которые хранятся в базе данных и могут быть вызваны из SQL-запросов. Они позволяют автоматизировать повторяющиеся задачи, такие как обработка данных или выполнение сложных запросов.

Между функциями и хранимыми процедурами в PostgreSQL есть несколько различий. Они показаны в таблице ниже.

| **Функции**                                                                                                               | **Хранимые процедуры**                                                             |
| ------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- |
| Функция имеет возвращаемый тип и возвращает значение                                                                      | Хранимая процедура не имеет возвращаемого типа, но имеет выходные аргументы        |
| Использование DML (insert, update, delete) запросов внутри функции невозможно. В функциях разрешены только SELECT-запросы | Использование DML-запросов (insert, update, delete) возможно в хранимой процедуре. |
| Функция не имеет выходных аргументов                                                                                      | Хранимая процедура имеет и входные, и выходные аргументы                           |
| Вызов хранимой процедуры из функции невозможно                                                                            | Использование или же управление транзакциями возможно в хранимой процедуре         |
| Вызов функции внутри `SELECT` запросов возможен                                                                           | Вызов хранимой процедуры из `SELECT` запросов невозможно                           |

## 1.5 Индексы

Индексы в PostgreSQL - специальные объекты базы данных, предназначенные в основном для ускорения доступа к данным. Это вспомогательные структуры: любой индекс можно удалить и восстановить заново по информации в таблице. Иногда приходится слышать, что СУБД может работать и без индексов, просто медленно. Однако это не так, ведь индексы служат также для поддержки некоторых ограничений целостности.

Несмотря на все различия между типами индексов (называемыми также _методами доступа_), в конечном счете любой из них устанавливает соответствие между ключом (например, значением проиндексированного столбца) и строками таблицы, в которых этот ключ встречается. Строки идентифицируются с помощью TID (tuple id), который состоит из номера блока файла и позиции строки внутри блока. Тогда, зная ключ или некоторую информацию о нем, можно быстро прочитать те строки, в которых может находиться интересующая нас информация, не просматривая всю таблицу полностью.

Важно понимать, что индекс, ускоряя доступ к данным, взамен требует определенных затрат на свое поддержание. При любой операции над проиндексированными данными - будь то вставка, удаление или обновление строк таблицы, - индексы, созданные для этой таблицы, должны быть перестроены, причем в рамках той же транзакции. Заметим, что обновление полей таблицы, по которым не создавались индексы, не приводит к перестроению индексов; этот механизм называется HOT (Heap-Only Tuples).

ПРИМЕР: Hash-индексирование в PostgreSQL обеспечивает более быстрый доступ по равенству, однако оно неэффективно для операций диапазона или сортировки. Создание Hash-индекса может быть полезно, если ваши операции часто включают точное сравнение на равенство.

Пример создания hash-индекса: `CREATE INDEX index_name ON table_name USING HASH (column_name);`

Этот пример применим, если у нас есть таблица "employees" и мы хотим создать hash-индекс для колонки "emp_id": `CREATE INDEX emp_id_index ON employees USING HASH (emp_id);`

```sql
CREATE INDEX - это оператор SQL, который используется для создания индекса.
emp_id_index - это имя, которое вы хотите присвоить индексу.
ON employees - это имя таблицы, для которой вы создаете индекс.
USING HASH - это указание PostgreSQL использовать hash-индекс.
(emp_id) - имя столбца, в котором вы хотите создать индекс.
```
- Учтите, что использование hash-индексов в PostgreSQL ограничено и не всегда рекомендуется. В текущих версиях PostgreSQL hash-индексы не могут быть воспроизведены потоковой репликацией, что затрудняет их использование в распределенных системах.

## 1.6 Триггеры

Триггеры - это специальные функции, которые автоматически выполняются при определенных событиях, таких как вставка, обновление или удаление данных. Они могут быть использованы для автоматического выполнения определенных задач, таких как обновление других таблиц или отправка уведомлений (INSERT, UPDATE, DELETE, TRUNCATE). В зависимости от требований мы можем запускать триггер до, после или вместо события/операции.

Это довольно мощный инструмент, у которого много сценариев использования. Вот лишь несколько примеров:

1. Вы можете использовать триггерные конструкции для отслеживания транзакций таблицы, регистрируя сведения о событии.
2. Вы можете создать триггер, с помощью которого будете проверять ограничения перед применением транзакции.
3. С помощью таких спусковых крючков вы можете автоматически заполнять поля, используя записи новых транзакций.

Создание функции триггера выглядит примерно так:

```sql
CREATE OR REPLACE FUNCTION my_trigger_function() 
RETURNS TRIGGER AS $$
BEGIN
  -- Здесь идёт код функции, который будет выполняться при срабатывании триггера
END;
$$ LANGUAGE plpgsql;
```

Прикрепление этой функции в качестве триггера к таблице:

```sql
CREATE TRIGGER my_trigger 
AFTER INSERT OR UPDATE OR DELETE ON my_table
FOR EACH ROW EXECUTE PROCEDURE my_trigger_function();
```

В этом примере `my_trigger` будет срабатывать каждый раз, когда в `my_table` происходит вставка, обновление или удаление строки.

Важно учесть, что функция триггера в PostgreSQL должна возвращать значение типа `TRIGGER`, а код функции обычно написан на языке `plpgsql`. Если функция успешно выполнена, это указывает на то, что операция, вызвавшая триггер, должна быть завершена. Если функция сгенерирует исключение, операция будет отклонена.