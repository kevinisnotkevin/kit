# KUBERNETES

## 1 Что такое Kubernetes?

**Kubernetes** - это оркестратор, который позволяет автоматизировать развертывание, масштабирование и управление контейнеризированными приложениями в кластере. Он самый популярный и поддерживается AWS, Google Cloud Platform, Microsoft Azure, Yandex Cloud и другими провайдерами облачных ресурсов.

Это решение было разработано инженерами Google - Джо Бедой, Бренданом Бернсом и Крейгом Мак-Лаки в 2014 году и вскоре после этого было выпущено с открытым исходным кодом. С тех пор Kubernetes быстро стал самостоятельной и активно развивающейся экосистемой "Cloud Native". В настоящее время управление Kubernetes осуществляется "Cloud Native Computing Foundation" (CNCF), подразделением Linux Foundation.

Kubernetes стал первым выпускным проектом CNCF и одним из самых быстрорастущих проектов с открытым исходным кодом в истории. Более 2300 участников работают над Kubernetes, и оно широко используется как крупными, так и малыми компаниями, включая половину Fortune 100.

**ПРЕИМУЩЕСТВА**

- Kubernetes автоматически развёртывает контейнеры приложений на узлах кластера, а также позволяет масштабировать их в зависимости от нагрузки.
- Kubernetes распределяет ресурсы (CPU, память, диск) между контейнерами приложений для эффективной работы.
- Kubernetes перезапускает контейнеры в случае их сбоя и распределяет нагрузку между репликами контейнеров.
- Kubernetes даёт гибко настраивать безопасность контейнеров приложений за счёт применения различных механизмов контроля доступа и шифрования трафика.
- Kubernetes поддерживает различные сетевые и дисковые плагины - можно выбрать наиболее подходящую под требования бизнеса реализацию сетевых подсистем и подсистем хранения персистентных данных.

## 2 Зачем использовать Kubernetes?

Контейнеры - хороший способ объединить и запустить приложения. В производственной среде необходимо управлять контейнерами, в которых выполняются приложения, и обеспечивать отсутствие простоев. Например, если контейнер выходит из строя, необходимо запустить другой контейнер. Не было бы проще, если бы такое поведение обрабатывалось системой?

Kubernetes предоставляет платформу для устойчивой работы распределенных систем. Он обеспечивает масштабирование и аварийное переключение приложения, предоставляет шаблоны развертывания и многое другое. Например: Kubernetes может легко управлять канареечным развертыванием вашей системы. При канареечном развертывании переключение происходит постепенно, начиная с части пользователей.

## 3 Возможности, которые предоставляет k8s

- **Обнаружение служб и балансировка нагрузки**

Kubernetes может предоставлять контейнер, используя DNS-имя или собственный IP-адрес. Если трафик в контейнере высокий, Kubernetes может балансировать нагрузку и распределять сетевой трафик, чтобы развертывание было стабильным.

- **Оркестрация хранилища**

Kubernetes позволяет автоматически монтировать систему хранения по выбору, например локальные хранилища, общедоступные облака и т. д.

- **Автоматизированное развертывание и откат**

Можно описать желаемое состояние развернутых контейнеров с помощью Kubernetes, и он может изменить фактическое состояние на желаемое с контролируемой скоростью. Например, можно автоматизировать Kubernetes для создания новых контейнеров для развертывания, удаления существующих контейнеров и переноса всех их ресурсов в новый контейнер.

- **Автоматическая упаковка контейнеров**

Вы предоставляете Kubernetes кластер узлов, которые он может использовать для выполнения контейнерных задач. Вы сообщаете Kubernetes, сколько процессора и памяти (ОЗУ) требуется каждому контейнеру. Kubernetes может разместить контейнеры на ваших узлах, чтобы максимально эффективно использовать ваши ресурсы.

- **Самовосстановление**

Kubernetes перезапускает контейнеры, которые вышли из строя, заменяет контейнеры, уничтожает контейнеры, которые не отвечают на заданную пользователем проверку работоспособности, и не объявляет о них клиентам, пока они не будут готовы к работе.

- **Управление секретами и конфигурациями**

Kubernetes позволяет хранить конфиденциальную информацию, такую как пароли, токены OAuth и ключи SSH, и управлять ею. Вы можете развертывать и обновлять секреты и конфигурацию приложения, не перестраивая образы контейнеров и не раскрывая секреты в конфигурации стека.

- **Пакетное выполнение**

Помимо сервисов, Kubernetes может управлять пакетными и CI-рабочими нагрузками, при желании заменяя вышедшие из строя контейнеры.

- **Горизонтальное масштабирование**

Масштабируйте свое приложение вверх и вниз с помощью простой команды, с помощью пользовательского интерфейса или автоматически в зависимости от загрузки ЦП.

- **Двойной стек IPv4/IPv6**

Выделение адресов IPv4 и IPv6 модулям и службам.

- **Расширяемость**

Добавляйте функции в свой кластер Kubernetes, не меняя исходного кода.


## 4 Чем Kubernetes не является?

Kubernetes - это не традиционная комплексная система PaaS (платформа как услуга). Поскольку Kubernetes работает на уровне контейнера, а не на уровне оборудования, он предоставляет некоторые общеприменимые функции, общие для предложений PaaS, такие как развертывание, масштабирование, балансировка нагрузки, а также позволяет пользователям интегрировать свои решения для ведения журнала, мониторинга и оповещения. Однако Kubernetes не является монолитным, и эти решения по умолчанию являются необязательными и подключаемыми. Kubernetes предоставляет стандартные блоки для создания платформ разработчиков, но сохраняет выбор пользователя и гибкость там, где это важно.

- Не ограничивает типы поддерживаемых приложений. Kubernetes стремится поддерживать различные виды рабочих нагрузок, включая рабочие нагрузки без сохранения состояния, с отслеживанием состояния и обработки данных. Если приложение может работать в контейнере, оно должно отлично работать и в Kubernetes.
- Не развертывает исходный код и не собирает ваше приложение. Рабочие процессы непрерывной интеграции, доставки и развертывания (CI/CD) предпочтениями организации, а также техническими требованиями.
- Не предоставляет службы уровня приложения, такие как промежуточное программное обеспечение (например, шины сообщений), платформы обработки данных (например, Spark), базы данных (например, MySQL), кэши или кластерные системы хранения (например, Ceph). как встроенные сервисы. Такие компоненты могут работать в Kubernetes и/или могут быть доступны приложениям, работающим в Kubernetes, через переносимые механизмы, такие как Open Service Broker.
- Не диктует решения для ведения логирования, мониторинга или оповещения. Он обеспечивает некоторые интеграции концепции, а также механизмы для сбора и экспорта метрик.
- Не предоставляет и не требует использования языка/системы конфигурации (например, Jsonnet). Он предоставляет декларативный API, на который могут быть нацелены произвольные формы декларативных спецификаций.
- Не предоставляет и не применяет какие-либо комплексные системы конфигурации, обслуживания, управления или самовосстановления машины.
- Кроме того, Kubernetes - это не просто система оркестровки. Фактически, он устраняет необходимость в оркестровке. Техническое определение оркестрации - это выполнение определенного рабочего процесса: сначала выполните A, затем B, затем C. Напротив, Kubernetes включает в себя набор независимых, компонуемых процессов управления, которые непрерывно приводят текущее состояние к заданному желаемому состоянию. Неважно, как вы доберетесь от А до С. Централизованное управление также не требуется. В результате система становится проще в использовании, становится более мощной, надежной, отказоустойчивой и расширяемой.

## 5 Краткая история Kubernetes

В книге "Site Reliability Engineering" описывается внутренний проект компании Google - система управления кластерами Borg. В 2014 году Google публикует исходные коды этого проекта и уже в 2015 году совместно с Linux Foundation создает фонд Cloud Native Computing Foundation (CNCF), передав в него исходные коды Kubernetes в качестве своего технического вклада. Фонд разрабатывает open-source проекты для создания приложений, ориентированных на облачные модели архитектур.

Сейчас Kubernetes - alumni, или по-русски «выпускник», Cloud Native Computing Foundation. Его довели до стабильной версии, и он получил статус Graduated Project (завершённый проект) в терминологии CNCF.

Первоначальные версии Kubernetes были более монолитными и адаптированными для работы с Docker на заднем плане. В рамках программы CNCF Kubernetes стал стабильным и расширяемым продуктом, и возможно изменение технологий практически на каждом уровне виртуальной инфраструктуры. Сейчас Kubernetes поддается высокой кастомизации: можно выбрать любую технологию работы с контейнерами, хранилищами или сетью.

Такой подход к развитию сделал Kubernetes популярным решением для продакшен-систем и корпоративных сетей. Он получил больше компонентов, связанных с безопасностью, алгоритмы управления ресурсами и процессами стали более стабильными.

## 6 Архитектура Kubernetes

![](/materials/images/devops/arch_k8s.png)

**Кластер**

Развертывание Kubernetes - это, в первую очередь, поднятие кластера. В кластере обязательно есть хотя бы одна нода. Она состоит из набора машин, которые запускают контейнерные приложения. Обычно на одной ноде кластер не ограничивается, поскольку дополнительные ноды решают проблему отказустойчивости.

**Nods (узлы)**

Это физические или виртуальные машины, на которых развертываются и запускаются контейнеры с приложениями. Каждый узел содержит сервисы, необходимые для запуска подов.

Обычно в кластере есть несколько узлов, но в среде обучения или среде с ограниченными ресурсами можно использовать один.

### 6.1 Типы нод

- **Master** (мастер-нода)

Узел, управляющий всем кластером. Он следит за остальными нодами и распределяет между ними нагрузку с помощью менеджера контроллеров (controller manager) и планировщика (scheduler). Как правило, мастер-нода занимается только управлением и не берет на себя рабочие нагрузки. Для повышения отказоустойчивости существует несколько мастер-нод.

- **Worker** (рабочие ноды)

Узлы, на которых работают контейнеры. В зависимости от параметров ноды (объема памяти и центрального процессора) на одном узле может работать множество контейнеров. Чем больше рабочих узлов, тем больше приложений можно запустить. Также количество влияет на отказоустойчивость кластера, потому что при выходе из строя одной ноды нагрузка распределяется по оставшимся.

Работающий кластер Kubernetes включает в себя агента, запущенного на нодах (kubelet), и компоненты мастера (APIs, scheduler, etc), поверх решения с распределенным хранилищем.

### 6.2 Pods (отсеки)

Pod определяется представлением запроса на запуск (execute) одного или более контейнеров на одном узле. Они разделяют доступ к таким ресурсам, как: тома хранилища и сетевой стек.

Поды считаются базовыми строительными блоками Kubernetes, поскольку все рабочие нагрузки в Kubernetes - например, Deployments, ReplicaSets и Jobs - могут быть выражены в виде pods. Pod - это единственный объект в Kubernetes, который приводит к запуску контейнеров. Нет пода - нет контейнера.

### 6.3 Control Plane (плоскость управления)

Компоненты плоскости управления принимают глобальные решения о кластере (например, планирование), а также обнаруживают события кластера и реагируют на них (например, запуск нового модуля, когда поле реплик развертывания не удовлетворено). Компоненты плоскости управления можно запускать на любой машине в кластере. Однако для простоты сценарии установки обычно запускают все компоненты плоскости управления на одном компьютере и не запускают на этом компьютере пользовательские контейнеры.

### 6.4 Объекты

Сущности, которые в архитектуре Kubernetes используются для представления состояния кластера.

**Объекты плоскости управления**

- **Kube-apiserver**

С помощью сервера API обеспечивается работа API кластера, обрабатываются REST-операции и предоставляется интерфейс, через который остальные компоненты взаимодействуют друг с другом. Кроме этого, через него проходят запросы на изменение состояния или чтение кластера.

- **Kube-scheduler**

Компонент-планировщик, который определяет на каких узлах разворачивать pods. Он учитывает такие факторы, как ограничения, требования к ресурсам, местонахождение данных и пр.

- **Etcd**

Распределенное хранилище в формате «ключ-значение». В нем хранится состояние всего кластера. Главная задача etcd - обеспечить отказоустойчивость кластера и консистентность данных. Etcd - самостоятельный проект. Он развивается отдельно от Kubernetes и применяется в разных продуктах.

- **Kube-controller-manager**

Компонент запускает работу контроллеров.

Логически каждый контроллер представляет собой отдельный процесс, но для уменьшения сложности все они компилируются в один двоичный файл и выполняются в одном процессе.

Существует много разных типов контроллеров. Некоторые примеры из них:

- _Контроллер узла:_ отвечает за обнаружение и реагирование при выходе узлов из строя.
- _Контроллер заданий:_ отслеживает объекты заданий, которые представляют собой одноразовые задачи, а затем создает модули для выполнения этих задач до завершения.
- _Контроллер EndpointSlice:_ заполняет объекты EndpointSlice (чтобы обеспечить связь между службами и модулями).
- _Контроллер ServiceAccount_: создайте учетные записи ServiceAccount по умолчанию для новых пространств имен.

- **Cloud-controller-manager**

Компонент плоскости управления Kubernetes, в который встроена логика управления, специфичная для облака. Диспетчер облачных контроллеров позволяет связать кластер с API облачного провайдера и отделить компоненты, которые взаимодействуют с этой облачной платформой, от компонентов, которые взаимодействуют только с кластером. Менеджер облачных контроллеров запускает только контроллеры, специфичные для облачного провайдера. Если вы используете Kubernetes у себя или в учебной среде на своем ПК, в кластере нет менеджера облачного контроллера.

**Объекты узлов**

- **Kubelet**

Cлужба управляет состоянием ноды: запуском, остановкой и поддержанием работы контейнеров и подов.

- **Kube-proxy**

Служба, которая управляет правилами балансировки нагрузки. Она конфигурирует правила IPVS или iptables, через которые выполняются проксирование и роутинг.

- **Container runtime**

Фундаментальный компонент, который позволяет Kubernetes эффективно запускать контейнеры. Он отвечает за управление выполнением и жизненным циклом контейнеров в среде Kubernetes.

Kubernetes поддерживает среды выполнения контейнеров, такие как Containerd, CRI-O и любую другую реализацию Kubernetes CRI (интерфейс выполнения контейнера).

## 7 Рабочие нагрузки

Рабочая нагрузка - это приложение, работающее в Kubernetes. Независимо от того, представляет ли ваша рабочая нагрузка один компонент или несколько, которые работают вместе, в Kubernetes вы запускаете ее внутри набора модулей. В Kubernetes Pod представляет собой набор работающих контейнеров в кластере.

### 7.1 Pods

Поды в кластере Kubernetes используются двумя основными способами:

- Поды, запускающие один контейнер

Модель «один контейнер на под» - наиболее распространенный вариант использования Kubernetes; в этом случае вы можете рассматривать Pod как оболочку одного контейнера; Kubernetes управляет подами, а не контейнерами напрямую.

- Поды, запускающие несколько контейнеров

Под может инкапсулировать приложение, состоящее из нескольких совместно расположенных контейнеров, которые тесно связаны и нуждаются в совместном использовании ресурсов. Эти совмещенные контейнеры образуют единое целое.

Группирование нескольких совместно расположенных и совместно управляемых контейнеров в одном модуле - относительно продвинутый вариант использования. Вам следует использовать этот шаблон только в определенных случаях, когда ваши контейнеры тесно связаны.

Вам не нужно запускать несколько контейнеров для обеспечения репликации (для обеспечения устойчивости или емкости)

**Храненение данных в подах**

Под может указать набор томов общего хранилища. Все контейнеры в модуле имеют доступ к общим томам, что позволяет этим контейнерам обмениваться данными. Тома также позволяют постоянным данным в поде сохраняться в случае, если один из контейнеров внутри потребуется перезапустить.

**Сети в подах**

Каждому поду назначается уникальный IP-адрес для каждого семейства адресов. Каждый контейнер в модуле использует общее сетевое пространство имен, включая IP-адрес и сетевые порты. Внутри пода (и только тогда) контейнеры, принадлежащие этому поду, могут взаимодействовать друг с другом, используя localhost. Когда контейнеры в поде взаимодействуют с объектами за пределами пода, они должны координировать использование общих сетевых ресурсов (таких как порты). Внутри пода контейнеры имеют общий IP-адрес и пространство портов и могут находить друг друга через localhost. Контейнеры в модуле также могут взаимодействовать друг с другом, используя стандартные межпроцессные коммуникации, такие как семафоры SystemV или общую память POSIX. Контейнеры в разных подах имеют разные IP-адреса и не могут обмениваться данными через IPC на уровне ОС без специальной настройки. Контейнеры, которые хотят взаимодействовать с контейнером, работающим в другом поде, могут использовать для связи IP-сети.

**Статус пода**

Поле статуса пода - это объект PodStatus, который имеет поле фазы.

Фаза пода - это простая, высокоуровневая сводка того, на каком этапе жизненного цикла находится модуль. Эта фаза не предназначена для комплексного сбора данных о состоянии контейнера или пода, а также не является комплексным конечным автоматом.

Вот возможные значения фазы:

| Pending   | Под был подтвержден кластером Kubernetes, но один или несколько контейнеров не настроены и не готовы к работе. Сюда входит время, которое модуль проводит в ожидании планирования, а также время, потраченное на загрузку образов контейнеров по сети. |
| --------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| Running   | Под привязан к узлу, и все контейнеры созданы. По крайней мере один контейнер все еще работает или находится в процессе запуска или перезапуска.                                                                                                       |
| Succeeded | Все контейнеры в модуле завершились успешно и не будут перезапущены.                                                                                                                                                                                   |
| Failed    | Все контейнеры в модуле завершили работу, и по крайней мере один контейнер завершил работу из-за сбоя. То есть контейнер либо завершил работу с ненулевым статусом, либо был завершен системой.                                                        |
| Unknown   | По какой-то причине состояние Pod не удалось получить. Эта фаза обычно возникает из-за ошибки связи с узлом, на котором должен работать модуль.                                                                                                        |

Если узел останавливается или отключается от остальной части кластера, Kubernetes применяет политику, устанавливающую для всех модулей Pod на потерянном узле значение Failed.

**Статус контейнера**

Помимо фазы пода в целом, Kubernetes отслеживает состояние каждого контейнера внутри пода. Вы можете использовать перехватчики жизненного цикла контейнера, чтобы запускать события в определенные моменты жизненного цикла контейнера.

| Waiting    | Если контейнер не находится ни в состоянии «Running», ни в состоянии «Terminated», он находится в состоянии «Waiting». Контейнер в состоянии ожидания по-прежнему выполняет операции, необходимые для завершения запуска: например, извлекает образ контейнера из реестра образов контейнера или применяет секретные данные. Когда вы используете `kubectl`для запроса пода с контейнером, находящимся в состоянии «Waiting», вы также видите поле «Причина», в котором указано, почему контейнер находится в этом состоянии. |
| ---------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Running    | Статус «Running» указывает на то, что контейнер выполняется без проблем. Если был настроен перехват postStart, он уже выполнен и завершен. Когда вы используете `kubectl` для запроса пода с запущенным контейнером, вы также видите информацию о том, когда контейнер перешел в состояние «Running».                                                                                                                                                                                                                         |
| Terminated | Контейнер в состоянии «Terminated» начал выполнение, а затем либо дошел до завершения, либо по какой-то причине вышел из строя. Когда вы используете `kubectl` для запроса пода с завершенным контейнером, вы видите причину, код выхода, а также время начала и окончания периода выполнения этого контейнера.<br><br>Если в контейнере настроен перехват preStop, этот перехват запускается до того, как контейнер перейдет в состояние «Terminated».                                                                       |

### 7.2 Управление рабочей нагрузкой

Kubernetes предоставляет несколько встроенных API для декларативного управления рабочими нагрузками и компонентами этих рабочих нагрузок.

В конечном итоге приложения запускаются как контейнеры внутри подов; однако управление отдельными подами потребует больших усилий. Например, если под выходит из строя, вы, вероятно, захотите запустить новый под, чтобы заменить его. Kubernetes может сделать это за вас.

Вы используете Kubernetes API для создания объекта рабочей нагрузки, который представляет более высокий уровень абстракции, чем под, а затем плоскость управления Kubernetes автоматически управляет объектами под от вашего имени на основе спецификации определенного вами объекта рабочей нагрузки.

- **Deployment**

Развертывание (и, косвенно, ReplicaSet) - наиболее распространенный способ запуска приложения в кластере. Развертывание хорошо подходит для управления рабочей нагрузкой приложений без отслеживания состояния в вашем кластере, где любой под в развертывании взаимозаменяем и может быть заменен при необходимости. (Развертывания являются заменой устаревшего API ReplicationController).

- **StatefulSet**

StatefulSet позволяет управлять одним или несколькими модулями (все они выполняют один и тот же код приложения), при этом поды полагаются на наличие отдельной идентичности. Это отличается от развертывания, в котором предполагается, что поды будут взаимозаменяемыми. Наиболее распространенное использование StatefulSet - возможность установить связь между его подами и их постоянным хранилищем. Например, вы можете запустить StatefulSet, который связывает каждый под с PersistentVolume. Если один из подов в StatefulSet выходит из строя, Kubernetes создает заменяющий под, подключенный к тому же самому поду PersistentVolume.

- **DaemonSet**

DaemonSet определяет поды, которые предоставляют возможности, локальные для определенного узла; например, драйвер, который позволяет контейнерам на этом узле получать доступ к системе хранения. Вы используете DaemonSet, когда драйвер или другая служба уровня узла должна запускаться на узле, где это полезно. Каждый под в DaemonSet выполняет роль, аналогичную системному демону на классическом сервере Unix/POSIX. DaemonSet может иметь основополагающее значение для работы вашего кластера, например, плагин, позволяющий этому узлу получить доступ к сети кластера, он может помочь вам управлять узлом или предоставить менее важные функции, улучшающие используемую вами контейнерную платформу. Вы можете запускать DaemonSets (и их модули) на каждом узле вашего кластера или только на его подмножестве (например, устанавливать драйвер ускорителя графического процессора только на узлах, на которых установлен графический процессор).

- Вы можете использовать _**Job**_ и/или _**CronJob**_ для определения задач, которые выполняются до завершения, а затем останавливаются. _**Job**_ представляет собой разовую задачу, тогда как каждое задание _**CronJob**_ повторяется по расписанию.

## 8 Хранение данных

### 8.1 Volumes (тома)

Файлы на диске в контейнере эфемерны, что создает некоторые проблемы для нетривиальных приложений при запуске в контейнерах. Одна проблема возникает, когда контейнер выходит из строя или останавливается. Состояние контейнера не сохраняется, поэтому все файлы, созданные или измененные за время существования контейнера, теряются. Во время сбоя kubelet перезапускает контейнер с чистым состоянием. Другая проблема возникает, когда в модуле работает несколько контейнеров, и им необходимо совместно использовать файлы. Может быть сложно настроить и получить доступ к общей файловой системе во всех контейнерах. Абстракция тома Kubernetes решает обе эти проблемы.

Управление хранилищем отличается от управления вычислительными экземплярами. Подсистема PersistentVolume предоставляет пользователям и администраторам API, который отделяет детали того, как предоставляется хранилище от того, как оно используется. Для этого представляются два новых ресурса API: PersistentVolume и PersistentVolumeClaim.

**PersistentVolume (PV)** - это часть хранилища в кластере, которая была предоставлена администратором или динамически предоставлена с использованием классов хранения. Это ресурс в кластере, точно так же, как узел является ресурсом кластера. PV - это плагины томов, такие как Volumes, но их жизненный цикл не зависит от какого-либо отдельного модуля, который использует PV. Этот объект API фиксирует детали реализации хранилища, будь то NFS, iSCSI или система хранения, зависящая от облачного провайдера.

**PersistentVolumeClaim (PVC)** - это запрос пользователя на хранилище. Это похоже на Pod. Поды потребляют ресурсы узла, а PVC - фотоэлектрические ресурсы. Поды могут запрашивать определенные уровни ресурсов (ЦП и Память). Утверждения могут запрашивать определенный размер и режимы доступа (например, их можно смонтировать ReadWriteOnce, ReadOnlyMany, ReadWriteMany или ReadWriteOncePod, см. раздел AccessModes).

Хотя PersistentVolumeClaims позволяют пользователю использовать абстрактные ресурсы хранилища, обычно для решения различных задач пользователям нужны PersistentVolumes с различными свойствами, такими как производительность. Администраторы кластера должны иметь возможность предлагать различные PersistentVolumes, которые различаются не только по размеру и режимам доступа, но не раскрывают пользователям подробности реализации этих томов. Для этих нужд существует ресурс StorageClass.

### 8.2 Configuration (конфигурация)

Передача параметров конфигурации приложениям, работающим в модулях, обычно выполняется с помощью ConfigMaps в кластерах Kubernetes. Эти параметры конфигурации сопоставляются в виде файлов с контейнерными приложениями.

Данные конфигурации не должны быть конфиденциальными. Любые конфиденциальные данные должны помещаться в Kubernetes Secrets (см. следующий раздел) или в какой-либо другой инструмент управления секретами, например HashiCorp Vault.

Хотя данные конфигурации не должны быть конфиденциальными, они все же могут быть чувствительными к безопасности. Например, изменение некоторых URL-адресов с https:// на http:// может привести к тому, что приложения будут отправлять конфиденциальную информацию без защиты TLS. Поэтому рекомендуется ограничить доступ к ConfigMaps в кластере только необходимому персоналу, по крайней мере, с правами на внесение изменений.

### 8.3 Kubernetes Secrets (Секреты)

Секреты - это объекты Kubernetes, аналогичные ConfigMaps, но содержащие конфиденциальные данные. Секреты должны быть зашифрованы в неактивном состоянии (обратите внимание, что вам может потребоваться включить шифрование ETCD вручную и убедиться, что ETCD взаимодействует только с сервером KubeAPI через взаимный TLS). И Secrets, и ConfigMaps используются для настройки рабочей нагрузки. Основная причина, по которой Kubernetes рассматривает их как отдельные объекты, заключается в том, что пользователи могут назначать разные права доступа для каждого типа объектов. Излишне говорить, что секреты Kubernetes должны быть максимально ограничены, и, в отличие от ConfigMaps, даже чрезмерные разрешения на чтение могут быть опасны, когда дело касается секретов.

Секреты можно передавать в контейнеры либо как переменные среды, либо как смонтированные файлы. Однако лучшие практики рекомендуют использовать файловый механизм и гарантировать, что секреты сопоставляются только с теми рабочими нагрузками, которые должны иметь к ним доступ.

### 8.4 Role-Based Access Control (Ролевой контроль доступа)

Объекты RBAC определяют, какие сущности (например, люди или модули) могут делать с различными объектами Kubernetes. Очень важно создать разумные RBAC (в идеале следуя принципу минимальных привилегий) для защиты вашего кластера. В противном случае шифрование томов и секретов будет бесполезным, если человек или модуль смогут получить доступ к API Kubernetes для таких объектов.

Как правило, если вы предоставляете слишком широкие разрешения, кто-то может легко создать модуль, который смонтирует секрет и, таким образом, получит легкий доступ к данным, которые вы хотели сохранить конфиденциальными. Разрешение на создание модулей здесь особенно важно, поэтому время, потраченное на создание правильно созданных правил RBAC, должно быть частью вашей стратегии по обеспечению безопасности хранящихся данных. ARMO Kubescape предлагает обширные возможности визуализации RBAC, позволяющие проверить, кто имеет доступ к какому объекту Kubernetes и какой тип доступа.

## 9 Службы, балансировка нагрузки и сеть

### 9.1 Сетевая модель Kubernetes

Каждый под в кластере получает свой собственный уникальный IP-адрес внутри кластера. Это означает, что вам не нужно явно создавать связи между подами, и вам почти никогда не придется иметь дело с пробросом портов контейнера с портами хоста. Это создает чистую, обратно совместимую модель, в которой поды можно рассматривать как виртуальные машины или физические хосты с точки зрения распределения портов, именования, обнаружения служб, балансировки нагрузки, настройки приложений и миграции.

Kubernetes предъявляет следующие фундаментальные требования к любой сетевой реализации (за исключением каких-либо преднамеренных политик сегментации сети):

- Поды могут взаимодействовать со всеми другими подами на любом другом узле без NAT.
- Агенты на узле (например, системные демоны, kubelet) могут взаимодействовать со всеми подами на этом узле.

Эта модель не только менее сложна в целом, но и в принципе совместима с желанием Kubernetes обеспечить легкий перенос приложений с виртуальных машин в контейнеры. Если инфраструктура располагалась на виртуальных машинах и виртуальные машины имела IP-адреса и могла взаимодействовать с другими виртуальными машинами в вашем проекте. Это та же базовая модель.

IP-адреса Kubernetes существуют в области пода - контейнеры внутри пода совместно используют свои сетевые пространства имен, включая IP-адрес и MAC-адрес. Это означает, что все контейнеры внутри пода могут достигать портов друг друга на локальном хосте. Это также означает, что контейнеры внутри пода должны координировать использование портов, но это ничем не отличается от процессов в виртуальной машине. Это называется моделью «IP на каждый модуль».

Как это реализовано, зависит от конкретной используемой среды выполнения контейнера.

4 основных концепции сетевой модели Kubernetes:

- Контейнеры внутри пода для коммуникации используют петлевой интерфейс.
- Кластерная сеть обеспечивает связь между различными подами.
- Service API позволяет сделать приложение, работающее в поде, доступным из-за пределов кластера.
    - Ingress обеспечивает дополнительные возможности специально для HTTP-приложений, веб-сайтов и API.
- Service также можно использовать для публикации служб только для использования внутри кластера.

### 9.2 Service

В Kubernetes Service - это метод предоставления доступа к сетевому приложению, которое работает как один или несколько подов в вашем кластере.

Основная цель Сервисов в Kubernetes - отсутствие необходимости модифицировать существующее приложение для использования незнакомого механизма обнаружения сервисов. Вы можете запускать код в подах, будь то код, разработанный для облака, или старое приложение, которое вы поместили в контейнер. Вы используете Сервис, чтобы сделать этот набор подов доступными в сети, чтобы клиенты могли с ним взаимодействовать.

Если вы используете развертывание для запуска своего приложения, это развертывание может динамически создавать и уничтожать поды. Время от времени вы не знаете, сколько из этих поды работает и исправно; возможно, вы даже не знаете, как называются эти поды. Поды Kubernetes создаются и уничтожаются в соответствии с желаемым состоянием вашего кластера. Поды — это эфемерные ресурсы (не следует ожидать, что отдельный под будет надежным и долговечным).

Каждый под получает свой собственный IP-адрес (Kubernetes ожидает, что это обеспечат сетевые плагины). Для данного развертывания в вашем кластере набор модулей, запущенных в один момент времени, может отличаться от набора модулей, запускающих это приложение мгновением позже.

Это приводит к проблеме: если некоторый набор подов (назовем их «бэкэндами») обеспечивает функциональность другим подам (назовем их «фронтендами») внутри вашего кластера, как внешние интерфейсы узнают и отслеживают, к какому IP-адресу подключаться, чтобы интерфейсная часть могла использовать внутреннюю часть рабочей нагрузки?

Ответом на этот вопрос являются Сервисы.

API Сервисы, являющийся частью Kubernetes, представляет собой абстракцию, помогающую предоставлять доступ к группам подов по сети. Каждый объект службы определяет логический набор конечных точек (обычно этими конечными точками являются поды), а также политику обеспечения доступа к этим подам.

Например, рассмотрим серверную часть обработки изображений без сохранения состояния, которая работает с 3 репликами. Эти реплики взаимозаменяемы - интерфейсным службам все равно, какой серверный интерфейс они используют. Хотя фактические поды, составляющие набор серверных частей, могут меняться, клиентским клиентам не нужно об этом знать, а также им не нужно самим отслеживать набор серверных частей.

Абстракция Службы обеспечивает такое разделение.

Набор подов, на которые нацелена Служба, обычно определяется определяемым вами селектором.

Если ваша рабочая нагрузка использует HTTP, вы можете использовать Ingress для управления тем, как веб-трафик достигает этой рабочей нагрузки. Ingress не является типом службы, но он действует как точка входа в ваш кластер. Ingress позволяет объединить правила маршрутизации в один ресурс, чтобы вы могли предоставлять доступ к нескольким компонентам вашей рабочей нагрузки, работающим отдельно в вашем кластере, за одним прослушивателем.

### 9.3 Ingress

Объект API, который управляет внешним доступом к службам в кластере, обычно HTTP.

Ingress может обеспечивать балансировку нагрузки, завершение запросов SSL и виртуальный хостинг на основе имени.

Ingrees предоставляет маршруты HTTP и HTTPS из-за пределов кластера службам внутри кластера. Маршрутизация трафика контролируется правилами, определенными в ресурсе Ingress.

Вот простой пример, когда Ingress отправляет весь свой трафик в одну службу:

![](/materials/images/devops/k8s_ingress.png)

Ingress может быть настроен для предоставления Службам URL-адресов, доступных извне, балансировки трафика, завершение запросов SSL/TLS и предложения виртуального хостинга на основе имени. Контроллер Ingress отвечает за выполнение Ingress, обычно с помощью балансировщика нагрузки, хотя он также может настроить пограничный маршрутизатор или дополнительные интерфейсы для обработки трафика.

### 9.4 Контроллер Ingrees

Чтобы ресурс Ingrees работал, в кластере должен быть запущен Ingrees контроллер.

В отличие от других типов контроллеров, которые запускаются как часть kube-controller-manager, контроллеры Ingress не запускаются автоматически вместе с кластером.

### 9.5 Сетевые политики

Если вы хотите контролировать поток трафика на уровне IP-адреса или порта для протоколов TCP, UDP и SCTP, вы можете рассмотреть возможность использования Kubernetes NetworkPolicies для определенных приложений в вашем кластере. NetworkPolicies - это конструкция, ориентированная на приложение, которая позволяет указать, как поду разрешено взаимодействовать с различными сетевыми «объектами» (здесь мы используем слово «объект», чтобы не перегружать более распространенные термины, такие как «конечные точки» и «сервисы») , которые имеют определенное значение Kubernetes). NetworkPolicies применяются к соединению с модулем на одном или обоих концах и не имеют отношения к другим соединениям.

Объекты, с которыми может взаимодействовать Pod, идентифицируются с помощью комбинации следующих трех идентификаторов:

1. Другие разрешенные поды (исключение: под не может заблокировать доступ к себе)
2. Разрешенные пространства имен
3. IP-блоки (исключение: трафик к и от узла, на котором работает под, всегда разрешен, независимо от IP-адреса пода или узла)

При определении NetworkPolicy на основе модуля или пространства имен вы используете селектор, чтобы указать, какой трафик разрешен к и от модулей, соответствующих селектору.

Между тем, когда создаются NetworkPolicies на основе IP, мы определяем политики на основе блоков IP (диапазонов CIDR).

### 9.6 DNS для служб и модулей

Kubernetes создает записи DNS для служб и модулей. Вы можете обращаться к Службам, используя согласованные DNS-имена вместо IP-адресов.

Kubernetes публикует информацию о модулях и службах, которая используется для программирования DNS. Kubelet настраивает DNS подов так, чтобы запущенные контейнеры могли искать службы по имени, а не по IP.

Службам, определенным в кластере, назначаются DNS-имена. По умолчанию список поиска DNS клиентского модуля включает собственное пространство имен модуля и домен кластера по умолчанию.

## 10 Сетевые плагины

В кластерной сети платформа Kubernetes 1.29 предлагает поддержку плагинов Container Network Interface (CNI). При выборе плагина CNI для использования в вашем кластере необходимо учесть его совместимость и соответствие ваших потребностей. В экосистеме Kubernetes доступны различные плагины, включая как с открытым, так и с закрытым исходным кодом.

Плагин CNI является неотъемлемой частью реализации сетевой модели Kubernetes. Рекомендуется использовать плагин CNI, который совместим с версией 0.4.0 или более новыми версиями спецификации CNI. Проект Kubernetes рекомендует использовать плагин, соответствующий спецификации CNI v1.0.0 (плагины могут быть совместимы с несколькими версиями спецификации).

### 10.1 Flannel

Flannel - это, по сути, оверлейная сеть, в которой данные TCP оборачиваются в другой сетевой пакет для маршрутизации и связи, и в настоящее время поддерживает маршрутизацию UDP, VxLAN, AWS VPC и GCE.

Flannel запускает агент flanneld на каждом хосте, который предварительно назначает хосту подсеть и назначает IP-адрес поду. flannel использует Kubernetes или etcd для хранения такой информации, как конфигурация сети, назначенные подсети и общедоступные IP-адреса хостов. Пакеты пересылаются через внутренние механизмы типа VXLAN, UDP или host-gw.

Flannel указывает, что отдельные поды на одном хосте принадлежат к одной и той же подсети, а поды на разных хостах принадлежат к разным подсетям.

Поддерживает 3 реализации: UDP, VxLAN, хост-gw.

- **Режим UDP**: использует устройство flannel.0 для декапсуляции пакетов, не поддерживается ядром изначально, частое переключение состояний ядра и пользователя, очень низкая производительность.
- **Режим VxLAN**: использует flannel.1 для распаковки пакетов, не поддерживается ядром изначально, с высокой производительностью.
- **Режим хост-gw**: нет необходимости в промежуточных устройствах, таких как flannel.1, прямой хост в качестве адреса следующего перехода подсети, лучшая производительность.

Потеря производительности Host-GW составляет около 10%, в то время как все другие сетевые решения на основе «туннелирования» VxLAN имеют потерю производительности около 20–30%.

### 10.2 Calico

Плагин Calico может использоваться в интеграции с Flannel (подпроект Canal) или самостоятельно, покрывая как функции по обеспечению сетевой связности, так и возможности управления доступностью.

Какие возможности даёт использование «коробочного» решения K8s и набора API из Calico?

Вот что встроено в NetworkPolicy:

- политики ограничены окружением;
- политики применяются к pod’ам, помеченным лейблами;
- правила могут быть применены к pod’ам, окружениям или подсетям;
- правила могут содержать протоколы, именованные или символьные указания портов.

А вот как Calico расширяет эти функции:

- политики могут применяться к любому объекту: pod, контейнер, виртуальная машина или интерфейс;
- правила могут содержать конкретное действие (запрет, разрешение, логирование);
- в качестве цели или источника правил может быть порт, диапазон портов, протоколы, HTTP- или ICMP-атрибуты, IP или подсеть (4 или 6 поколения), любые селекторы (узлов, хостов, окружений);
- дополнительно можно регулировать прохождение трафика с помощью настроек DNAT и политик проброса трафика.

## 11 Что такое Helm и Helm Charts?

Helm - это инструмент для управления Kubernetes, который автоматизирует создание, упаковку, настройку и развертывание приложений и служб в кластерах Kubernetes.

### 11.1 Что такое Helm?

Если бы Kubernetes был операционной системой, то helm был бы его менеджером пакетов, аналогом apt для ubuntu и yum для centos.

Helm развертывает пакетные приложения в Kubernetes и структурирует их в чарты (Helm Charts). Чарты содержат все предустановленные ресурсы приложения вместе со всеми версиями, которые помещены в один легко управляемый пакет.

Helm упрощает установку, обновление, вызов зависимостей и настройку развертываний в Kubernetes с помощью простых CLI-команд. Пакеты программного обеспечения находятся в репозиториях или создаются.

### 11.2 Почему нам нужен Helm?

Helm упрощает установку, обновление, управление зависимостями и настройку развертываний в Kubernetes с помощью простых cli-команд. Helm автоматизирует обслуживание YAML-файлов для объектов Kubernetes, упаковывая информацию в чарты и анонсируя их в кластере Kubernetes.

Helm позволяет отслеживать историю версий для установленных приложений, а также выполнять откат к предыдущим версиям или обновление до новых.

Доступные команды:

- **Completion** — создает сценарий автозаполнения для указанной оболочки.
- **Create** — создает новый чарт с заданным именем.
- **Dependency** — управление зависимостями чарта.**Env** — информация о клиентской среде Helm.
- **Get** — загрузка расширенной информации об именованном релизе.
- **Help** — помощь по любой команде.
- **History** — получить историю релизов.
- **Install** — установить чарт.
- **Lint** — проверить чарт на возможные проблемы.
- **List** — список релизов.
- **Package** — упаковать каталог чарта в архив чарта.
- **Plugin** — установить, внести в список или удалить плагины Helm.
- **Pull** — загрузить чарт из репозитория или (опционально) распаковать его в локальный каталог.
- **Repo** — установка, внесение в список, удаление, обновление и индексация репозиториев чартов.
- **Rollback** — откат релиза к предыдущей версии.
- **Search** — поиск в чарте по ключевым словам.
- **Show** — показать информацию о чарте.
- **Status** — отображение статуса названного релиза.
- **Template** — локальное отображение шаблонов.
- **Test** — запустить тесты релиза.**Uninstall** — деинсталлировать релиз.
- **Upgrade** — обновить релиз.
- **Verify** — проверить, что чарт по указанному пути подписан и действителен.
- **Version** — распечатать информацию о версии клиента.

### 11.3 Что вы можете сделать с помощью Helm?

Helm позволяет разработчикам программного обеспечения развертывать и тестировать среду самым простым способом. Требуется меньше времени, чтобы перейти от разработки к тестированию и продакшену.

Помимо повышения производительности, Helm предоставляет разработчикам удобный способ упаковки и отправки приложений конечным пользователям для установки.

### 11.4 Как работает Helm?

Helm и Kubernetes работают как клиент-серверное приложение. Клиент Helm отправляет ресурсы в кластер Kubernetes. Серверная часть зависит от версии: Helm 2 использует **Tiller**, тогда как Helm 3 избавился от **Tiller** и полностью полагается на Kubernetes API.

![](/materials/images/devops/k8s_helm.png)

### 11.5 Что такое Helm Charts?

Чарты Helm (Helm Charts) - это пакеты Helm, состоящие из файлов и шаблонов YAML, которые преобразуются в файлы манифеста Kubernetes. Чарты могут повторно использоваться кем угодно и в любой среде, что уменьшает сложность и количество дубликатов.

### 11.6 Как работают чарты Helm?

Три основные концепции чартов Helm:

1. **Чарт** - предварительно настроенный шаблон ресурсов Kubernetes.
2. **Релиз** - чарт, развернутый с помощью Helm в кластере Kubernetes.
3. **Репозиторий** - общедоступные чарты.

Рабочий процесс заключается в поиске чартов через репозитории и создании релизов путем установки чартов в кластеры Kubernetes.

### 11.7 Структура чарта Helm

Файлы и каталоги чарта Helm имеют определенную функцию:

|**Название**|**Тип**|**Функция**|
|---|---|---|
|charts/|Каталог|Каталог для управляемых вручную зависимостей чарта.|
|templates/|Каталог|Написанные на языке Go файлы шаблонов, объединенные с конфигурационными значениями из файла values.yaml и предназначенные для генерации манифестов Kubernetes.|
|Chart.yaml|Файл|Метаданные о чартах, такие как: версия, имя, ключевые слова для поиска и так далее.|
|LICENSE (опционально)|Файл|Лицензия на чарт в текстовом формате.|
|[README.md](http://README.md) (опционально)|Файл|Удобочитаемая информация для пользователей чарта.|
|requirements.yaml (опционально)|Файл|Список зависимостей чарта.|
|values.yaml|Файл|Настройки чарта по умолчанию.|

### 11.8 Репозитории чартов Helm

Репозитории содержат чарты, которые могут быть установлены или предоставлены для доступа другим пользователям. Helm обеспечивает поиск напрямую из клиента. Существует два основных типа поиска:

- `helm search hub` - поиск через Artifact Hub  из множества репозиториев.
- `helm search repo` - поиск через репозитории, добавленные в локальном клиенте Helm с помощью helm repo add.

Без каких-либо фильтров в результатах поиска отображаются все доступные чарты. Чтобы уточнить запрос, добавьте условие поиска. Например:

```sql
helm search hub wordpress
```

Когда найдете подходящий чарт, установите его с помощью `helm install`**.**

### 11.9 Релизы чартов

При установке чарта создается новый пакет. Команда `helm install` принимает два аргумента:

```sql
helm install <release name> <chart name>
```