# Известные файлы: robots.txt, htaccess, sitemap и др.

## robots.txt

robots txt – это обыкновенный текстовый документ, который лежит в корне веб-сайта и информирует поисковых роботов о том, какие страницы и файлы они должны сканировать и индексировать, а для каких наложен запрет.

Без этого файла поисковики будут хаотично блуждать по сайту, сканировать и индексировать буквально все подряд: дубли, служебные документы, страницы с текстами «заглушками» (Lorem Ipsum) и тому подобное.

Правильный robots txt не дает такому происходить и буквально ведет роботов по сайту, подсказывая, что разрешено индексировать, а что необходимо упустить.

Существуют специальные директивы robots.txt для данных задач:

- Allow - допускает индексацию
- Disallow - запрещает индексацию

Пояснения:

Агенту пользователя с названием Googlebot запрещено сканировать любые URL, начинающиеся с `example.com/nogooglebot`. Любым другим агентам пользователя разрешено сканировать весь сайт. Это правило можно опустить, и результат будет тем же. По умолчанию агенты пользователя могут сканировать сайт целиком. Файл Sitemap этого сайта находится по адресу `example.com/sitemap.xml`.

## htaccess

Файл htaccess – конфигурационный файл веб-сервера Apache уровня каталога. Он содержит набор инструкций, обуславливающих принципы работы ПО Apache в рамках текущего каталога и дочерних папках без внесения изменений в общие настройки веб-сервера.

Для простоты понимания рассмотрим основные принципы работы веб-приложений в рамках протоколов http и https (например, обычные сайты). Сам сайт представляет собой в наиболее простом виде совокупность контента и программного кода. Последний используется как логика доступа к контенту. То есть, это набор мультимедийных (изображения, аудио, видео) файлов, а также текстовых файлов, содержащих код. Все эти файлы записывают в определенный каталог компьютера, который называют сервером.

Однако, одной программной логики самого сайта недостаточно для взаимодействия с ним. На серверах хранятся сразу несколько веб-приложений, и для одновременной и бесперебойной работы каждого из них требуется ПО более высокого уровня, которое берет на себя задачи по управлению сразу всеми запросами, поступающими от клиентов (обычных пользователей интернета). В качестве такого ПО выступает Apache.

Этот популярный веб-сервер поставляется в виде ПО с открытыми исходными кодами и обеспечивает возможность внесения тонких настроек в его функционал с помощью различных конфигурационных файлов. htaccess – это один из таких файлов. Он выполняет следующие основные функции:

- Переопределение директив (команд, обуславливающих принципы работы веб-сервера), которые хранятся в основном конфигурационном файле – httpd.conf, для локальных нужд (для определенного каталога).
- Настройка работы интерпретатора PHP (некоторые важные функции).
- Использование для ограничения доступа к некоторым каталогам.
- Выбор кодировки символов.
- Замена сложных URL на более простые и понятные (ЧПУ).
- Указание директив простого, сложного перенаправления при навигации по сайту и многое другое.

## sitemap

Файл sitemap называют картой сайта – она помогает ориентироваться поисковым роботам среди папок и документов вашего ресурса.

Файл Sitemap расположен в корне сайта и содержит данные о его страницах. В карте сайта должны быть ссылки на все страницы сайта. Поисковые роботы Яндекса и Гугла заходят на ваш сайт и гуляют по нему до тех пор, пока не кончится рабочий день. Рабочий день у робота — это краулинговый бюджет. То есть объем страниц и файлов, которые робот может изучить за определенный период времени. Чтобы не задерживать робота и в то же время улучшить индексацию, помогает карта сайта.

```xml
<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="<http://www.sitemaps.org/schemas/sitemap/0.9>">

    <url>

        <loc><http://www.example.com/page1.html></loc>

        <lastmod>2005-01-01</lastmod>

        <changefreq>monthly</changefreq>

        <priority>0.8</priority>

    </url>

    ...

</urlset>

```
- Пример файла